# 🧠 深度学习与 PyTorch 核心实战知识库

> **编者按**：这份文档记录了从张量维度、网络构建、防过拟合到数值稳定性与底层的初始化推导。它不仅仅是代码 API 的堆砌，更是对神经网络运作机制的“物理直觉”总结。

## 模块一：数据张量与工程规矩

### 1. 数据的形状 vs 特征的维度

很多初学者容易混淆“数组是几维的”和“特征有多少个”。

* **规矩**：PyTorch 中的 `nn.Linear` 永远要求输入数据呈现二维表格的形状：`[batch_size, num_features]`（批次大小，特征数量）。

* **实操**：如果只有一个特征（比如一维坐标 $x$），即使数据是一维数组 `[20]`，也必须使用 `unsqueeze(1)` 将其扩展为 `[20, 1]`。

* **理解**：`nn.Linear(1, 100)` 关心的永远是**列数（特征数=1）**，而不是数据的总维度。

### 2. PyTorch 与 NumPy 的“安全跨界”

Matplotlib 等数据科学生态只认识 NumPy 数组，不认识 PyTorch 的张量。

* **防弹三连招**：`tensor.cpu().detach().numpy()`

  * `.cpu()`：将数据从 GPU 显存拉回 CPU 内存。

  * `.detach()`：残忍剥离计算图（隐形书记员），抹除梯度追踪，防止报错。

  * `.numpy()`：最终的形态转换。

## 模块二：网络架构与非线性灵魂

### 1. 隐藏层的微观流水线

一个标准的隐藏层是由 `纯线性乘加 (nn.Linear)` + `非线性激活 (nn.ReLU)` 组成的。

* **没有激活函数会怎样？** 连续的线性层在数学上会坍缩成一个单层，无论堆叠多深，都只能画出直线/平面，失去拟合复杂世界的能力。

### 2. 常用激活函数全景指南

激活函数不仅提供非线性，还直接决定了梯度的流动情况。

* **Sigmoid**

  * **公式**：$f(x) = \frac{1}{1 + e^{-x}}$

  * **特征**：将输入压缩到 $(0, 1)$ 之间。

  * **缺陷**：① 导数最大仅为 $0.25$，深层极易导致**梯度消失**；② 输出不以 $0$ 为中心（均值 $>0$），导致后续层的输入发生偏移。

  * **代码**：`nn.Sigmoid()`（现在多用于二分类的输出层，隐藏层已弃用）。

* **Tanh (双曲正切)**

  * **公式**：$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

  * **特征**：将输入压缩到 $(-1, 1)$ 之间，且以 $0$ 为中心。

  * **地位**：Sigmoid 的进阶版，配合 Xavier 初始化曾拯救了浅层网络，但在极大/极小值处仍存在梯度消失。

  * **代码**：`nn.Tanh()`。

* **ReLU (Rectified Linear Unit)**

  * **公式**：$f(x) = \max(0, x)$

  * **特征**：正数区间导数恒为 **1**。打破了连乘衰减魔咒，充当“梯度高速公路”，计算极快。

  * **缺陷**：“死神经元”问题，一旦落入负数区，梯度永远为 0，不再更新。

  * **代码**：`nn.ReLU()`（现代深度学习最默认、最主流的选择）。

* **Leaky ReLU & PReLU**

  * **公式**：$f(x) = \max(\alpha x, x)$ （通常 $\alpha=0.01$）

  * **特征**：给负数区域保留了一个极小的斜率 $\alpha$，解决了 ReLU 死神经元的问题（PReLU 则是让 $\alpha$ 变成可学习参数）。

  * **代码**：`nn.LeakyReLU(negative_slope=0.01)`。

### 3. 高阶视角：用泰勒展开（Taylor Expansion）审视激活函数

> **核心直觉**：由于模型初始化时权重都在 $0$ 附近，因此数据在最开始传播时，输入值 $x$ 极其接近于 $0$。**激活函数在** $x=0$ **附近的表现，直接决定了模型能不能顺利挺过最初的训练阶段！**

我们希望在 $0$ 附近，激活函数能做到 $f(x) \approx x$（像透明玻璃一样原样穿透），这样才不会破坏 Xavier 初始化的方差平衡。我们用麦克劳林级数（在 $x=0$ 处的泰勒展开）来照妖：

**① Sigmoid 的泰勒展开：**

$$
\sigma(x) \approx \frac{1}{2} + \frac{1}{4}x - \frac{1}{48}x^3 + \dots
$$

* **致命缺陷暴露**：

  * 常数项是 $\frac{1}{2}$ 而不是 $0$！这证明了它**不以零为中心**，强行给下一层塞入了 $0.5$ 的偏置。

  * 线性项系数是 $\frac{1}{4}$！这说明在 $0$ 附近，它把信号强度直接压缩了 $4$ 倍，方差被压缩了 $16$ 倍，**加速了梯度消失**！

**② Tanh 的泰勒展开：**

$$
\tanh(x) \approx x - \frac{1}{3}x^3 + \dots
$$

* **完美开局**：常数项为 $0$（零中心），线性项系数完美等于 $1$！所以在 $0$ 附近，$\tanh(x) \approx x$。这就解释了**为什么 Xavier 初始化配 Tanh 能够如此完美地保持方差一致**。

**③ 极其精妙的改造例子：**$4 \times \text{sigmoid}(x) - 2$
如果非要用 Sigmoid，我们能不能通过数学改造拯救它？你可以看看这个神奇的函数：$g(x) = 4\sigma(x) - 2$。
我们把 $\sigma(x)$ 的泰勒展开代进去：

$$
g(x) \approx 4 \left(\frac{1}{2} + \frac{1}{4}x\right) - 2 = 2 + x - 2 = x
$$

* **奇迹出现**：展开后的结果极其干净地变成了 $g(x) \approx x$！

* **原理解密**：减去 $2$ 消除了不以 $0$ 为中心的痼疾（抵消了常数项）；乘以 $4$ 强行把原本只有 $0.25$ 的最大斜率拔高到了 $1$（修正了方差缩水）。经过简单的缩放和平移，我们用数学强行把一坨烂泥捏成了符合 $f(x) \approx x$ 完美特性的神仙函数！

### 4. 动态网络拼装（工程魔法）

利用 Python 的解包特性 `*` 动态组装 `nn.Sequential`：

```python
layers = [nn.Linear(1, 100), nn.ReLU()]
if use_dropout:
    layers.append(nn.Dropout(0.2))
model = nn.Sequential(*layers)   #将列表解包
```
## 模块三：应对过拟合的“正则化双雄”

过拟合的本质是模型过于复杂，死记硬背了数据中的噪音。我们需要给它“戴上镣铐”。

### 1. 丢弃法 (Dropout) —— 破坏共适应性

* **机制**：训练时以概率 $p$ 随机让部分神经元“断电”（归零），强迫模型不能依赖个别神经元，只能学习全局特征。

* **代码**：`nn.Dropout(p)`，通常紧跟在激活函数之后。

* **状态切换（极其致命）**：

  * `net.train()`：开启随机丢弃。

  * `net.eval()`：关闭丢弃，所有神经元参与预测（考试模式）。

### 2. 权重衰退 (Weight Decay) —— 限制参数膨胀

* **机制**：通过在损失函数中加入 L2 正则项，强迫模型在更新时不断压缩权重的绝对值。权重越小，拟合出的曲线越平滑，越不容易被噪音带偏。

* **代码**：无需修改模型，直接在优化器中设置：`torch.optim.Adam(..., weight_decay=0.01)`。

### 3. 科研级对比实验

* **绝对公平**：使用 `torch.manual_seed(42)` 固定随机种子，保证控制变量法严格成立。

## 模块四：数值稳定性与模型初始化

这是保证深层网络能够“活下来”的生命维持系统。核心目标：**保证信号在正向和反向传播时，每一层的方差（信号强度）保持一致。**

### 1. Xavier (Glorot) 初始化

* **痛点**：解决无激活函数或 Tanh/Sigmoid 激活函数下的信号衰减。

* **数学原理**：为了兼顾正向传播和反向传播，权重的方差设定为输入与输出神经元数量的调和平均：$\text{Var}(w) = \frac{2}{n_{in} + n_{out}}$。

* **代码**：`nn.init.xavier_normal_(m.weight)`

### 2. He (Kaiming) 初始化

* **痛点**：ReLU 会无情抹零所有负数，导致方差直接**减半**。Xavier 初始化在 ReLU 面前会全线崩溃（方差迅速趋零）。

* **数学原理**：既然 ReLU 砍掉了一半方差，那就在初始化时直接把方差**乘以 2** 进行暴力补偿：$\text{Var}(w) = \frac{2}{n_{in}}$。

* **代码**：`nn.init.kaiming_normal_(m.weight, nonlinearity='relu')`

### 3. 理论与现实的缝隙（方差漂移）

即使使用了 He 初始化，经过 30 层 ReLU 后方差依然会轻微跌落（如跌至 0.39）。

* **原因**：ReLU 截断负数后，破坏了理想的完美正态分布形状，且特征维度是有限的（抽样误差），导致并非绝对精准地切除 50% 方差。

* **行业终极解法**：在网络中加入 **BatchNorm / LayerNorm** 层，直接在运行时强行矫正均值和方差，彻底终结数值不稳定。